{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact with df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be loaded in through a CSV, JSON, XML, or a Parquet file. \n",
    "\n",
    "It can also be created using an existing RDD and through any other database, like Hive or Cassandra as well. \n",
    "\n",
    "It can also take in data from HDFS or the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import time\n",
    "import operator\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"spark-basic\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first example with 'fake' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6 },\n",
    "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William', 5: 'Jack'},\n",
    "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male', 5: 'male'},\n",
    "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0, 5: 0}}\n",
    "\n",
    "data2 = {'Id': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
    "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
    "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
    "\n",
    "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
    "df2_pd = pd.DataFrame(data2, columns=data2.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a spark session to process this type of data. Caution, many examples on the Internet do not specify what \"spark.\" is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame(df1_pd)\n",
    "df2 = spark.createDataFrame(df2_pd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the properties of our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Survived: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating with dataframes\n",
    "\n",
    "If you have experience coding with the package dplyr from R, you have more than half of it covered!\n",
    "\n",
    "**Filter**, **select**, **mutate**, **summarize** and **arrange** are the basic verbs here too, although some of them have different names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    Name|   Sex|\n",
      "+--------+------+\n",
      "|    Owen|  male|\n",
      "|Florence|female|\n",
      "|   Laina|female|\n",
      "|    Lily|female|\n",
      "| William|  male|\n",
      "|    Jack|  male|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select([\"Name\",\"Sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.Sex == 'female').filter(df1.Survived == 1).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also do it SQL-style. Note that this is not a python syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(\"Sex='female'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the equivalent of mutate...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hello = df2.withColumn('PricePerYear', df2.Fare/df2.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: bigint, Age: bigint, Fare: double, Pclass: bigint, PricePerYear: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby is needed when we want to summarize our data... the use is identical than in lists or dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').avg(*[\"Age\",\"Fare\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-----------------+\n",
      "|Pclass|          avg(Age)|        avg(Fare)|\n",
      "+------+------------------+-----------------+\n",
      "|     1|              36.5|             62.2|\n",
      "|     3|27.666666666666668|7.733333333333333|\n",
      "+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg({'Age': 'avg', 'Fare': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with the syntax *****, it refers to \"unpack\", so we have to provide an unpacked list.\n",
    "\n",
    "If we want multiple functions at the same time, we need an dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+---------+\n",
      "|Pclass|count(1)|          avg(Age)|sum(Fare)|\n",
      "+------+--------+------------------+---------+\n",
      "|     1|       2|              36.5|    124.4|\n",
      "|     3|       3|27.666666666666668|     23.2|\n",
      "+------+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pclass').agg({'*': 'count', 'Age': 'avg', 'Fare': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present the summarized data in a nicer format, we can un toDF() to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+----------+\n",
      "|Pclass|counts|       average_age|total_fare|\n",
      "+------+------+------------------+----------+\n",
      "|     1|     2|              36.5|     124.4|\n",
      "|     3|     3|27.666666666666668|      23.2|\n",
      "+------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  (\n",
    "    df2.groupby('Pclass').agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
    "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
    "    .show()\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can **arrange** dataframes using the method sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+------+\n",
      "| Id|Age|Fare|Pclass|\n",
      "+---+---+----+------+\n",
      "|  1| 22| 7.3|     3|\n",
      "|  3| 26| 7.9|     3|\n",
      "|  4| 35|53.1|     1|\n",
      "|  5| 35| 8.0|     3|\n",
      "|  2| 38|71.3|     1|\n",
      "+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.sort('Age', ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good reference: https://dzone.com/articles/pyspark-dataframe-tutorial-introduction-to-datafra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic joins and unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df2.Id == df1.PassengerId).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it symmetric? Where is Jack? :'(\n",
    "\n",
    "If you have some time, explore with other types of join, such as \"left\", \"right\", \"cross\", \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And union doesn't do what we could expect... why would we need it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+\n",
      "|PassengerId|    Name|   Sex|Survived|\n",
      "+-----------+--------+------+--------+\n",
      "|          1|    Owen|  male|       0|\n",
      "|          2|Florence|female|       1|\n",
      "|          3|   Laina|female|       1|\n",
      "|          4|    Lily|female|       1|\n",
      "|          5| William|  male|       0|\n",
      "|          6|    Jack|  male|       0|\n",
      "|          1|      22|   7.3|       3|\n",
      "|          2|      38|  71.3|       1|\n",
      "|          3|      26|   7.9|       3|\n",
      "|          4|      35|  53.1|       1|\n",
      "|          5|      35|   8.0|       3|\n",
      "+-----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weird join procedure... what is going on here? Are you able to find why would it be useful? (and dangerous?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          1|    Owen|  male|       0|  2| 38|71.3|     1|\n",
      "|          1|    Owen|  male|       0|  3| 26| 7.9|     3|\n",
      "|          1|    Owen|  male|       0|  4| 35|53.1|     1|\n",
      "|          1|    Owen|  male|       0|  5| 35| 8.0|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          2|Florence|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  4| 35|53.1|     1|\n",
      "|          2|Florence|female|       1|  5| 35| 8.0|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          3|   Laina|female|       1|  4| 35|53.1|     1|\n",
      "|          3|   Laina|female|       1|  5| 35| 8.0|     3|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "|          4|    Lily|female|       1|  5| 35| 8.0|     3|\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.PassengerId <= df2.Id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... what about doing joins with SQL?\n",
    "\n",
    "If we want to do that, we will need temporary Views! Sounds worse than it actually is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|PassengerId|    Name|   Sex|Survived| Id|Age|Fare|Pclass|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "|          5| William|  male|       0|  5| 35| 8.0|     3|\n",
      "|          1|    Owen|  male|       0|  1| 22| 7.3|     3|\n",
      "|          3|   Laina|female|       1|  3| 26| 7.9|     3|\n",
      "|          2|Florence|female|       1|  2| 38|71.3|     1|\n",
      "|          4|    Lily|female|       1|  4| 35|53.1|     1|\n",
      "+-----------+--------+------+--------+---+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView('df1_temp')\n",
    "df2.createOrReplaceTempView('df2_temp')\n",
    "\n",
    "query = '''\n",
    "    select *\n",
    "    from df1_temp a, df2_temp b\n",
    "    where a.PassengerId = b.Id'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------+\n",
      "|Clase|             Media|round(Media, 2)|\n",
      "+-----+------------------+---------------+\n",
      "|    1|              36.5|           36.5|\n",
      "|    3|27.666666666666668|          27.67|\n",
      "+-----+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, col\n",
    "\n",
    "resumen = df2.groupby('Pclass').avg(\"Age\").toDF(\"Clase\",\"Media\")\n",
    "resumen.select(\"*\",round(col(\"Media\"),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indirectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod = pd.read_excel(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.xlsx\",sheet_name=\"Products\")\n",
    "cust = pd.read_excel(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.xlsx\",sheet_name=\"Customers\")\n",
    "purc = pd.read_excel(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.xlsx\",sheet_name=\"Purchases\")\n",
    "sell = pd.read_excel(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.xlsx\",sheet_name=\"Sellers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod = spark.createDataFrame(prod)\n",
    "cust = spark.createDataFrame(cust)\n",
    "purc = spark.createDataFrame(purc)\n",
    "sell = spark.createDataFrame(sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+--------+------+\n",
      "|Code|Buyer|Product|Quantity|Seller|\n",
      "+----+-----+-------+--------+------+\n",
      "|   1|  139|     14|       7|    33|\n",
      "|   2|  127|     51|       3|    17|\n",
      "|   3|  137|     10|      15|    22|\n",
      "|   4|   58|     54|       3|    30|\n",
      "|   5|  196|     57|       8|    11|\n",
      "|   6|  135|     19|       6|     3|\n",
      "|   7|   28|     59|      19|    10|\n",
      "|   8|  193|     60|      18|    18|\n",
      "|   9|   68|      5|       3|    33|\n",
      "|  10|   30|     51|      12|     3|\n",
      "|  11|   80|      8|       9|    29|\n",
      "|  12|   90|     12|       3|    12|\n",
      "|  13|   72|     30|       7|    38|\n",
      "|  14|  174|     53|      11|    15|\n",
      "|  15|  167|     35|      11|    45|\n",
      "|  16|  125|     13|      16|    21|\n",
      "|  17|  120|     24|       1|    20|\n",
      "|  18|  141|     28|      16|    13|\n",
      "|  19|  198|     35|      11|    49|\n",
      "|  20|   42|     32|       6|    52|\n",
      "+----+-----+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+\n",
      "|Code|                Name|Price|\n",
      "+----+--------------------+-----+\n",
      "|   1|    American cheeses| 0,62|\n",
      "|   2|Appellation d'Ori...|15,58|\n",
      "|   3|     Apple cultivars| 15,8|\n",
      "|   4|        Bacon dishes| 15,6|\n",
      "|   5|   Bacon substitutes| 5,22|\n",
      "|   6|     Basil cultivars| 2,21|\n",
      "|   7|              Breads|14,87|\n",
      "|   8| Breakfast beverages| 2,17|\n",
      "|   9|   Breakfast cereals| 2,27|\n",
      "|  10|     Breakfast foods| 5,51|\n",
      "|  11|     British cheeses|14,68|\n",
      "|  12|               Cakes|16,66|\n",
      "|  13|             Candies|11,25|\n",
      "|  14|             Cheeses| 4,43|\n",
      "|  15|        Cheese soups| 7,21|\n",
      "|  16|Christmas dishes ...| 3,06|\n",
      "|  17|           Cocktails| 8,55|\n",
      "|  18|             Cookies| 3,16|\n",
      "|  19|Dishes using coco...|16,58|\n",
      "|  20|               Diets|17,72|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"C:/Users/joangj/OneDrive - NETMIND/BTS/2020/data/FoodMarket.csv\",sep=\";\",header = True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you solve the problem of the decimal format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the file back again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way, but generates a new folder and extra files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/joangj/OneDrive - NETMIND/BTS/2020/RealTime/Newdata already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-33ef55dc9da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Newdata\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1030\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/joangj/OneDrive - NETMIND/BTS/2020/RealTime/Newdata already exists.;"
     ]
    }
   ],
   "source": [
    "prod.write.csv(\"Newdata\",header = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many partitions do we have in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purc.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = purc.groupBy(\"Seller\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Seller|count|\n",
      "+------+-----+\n",
      "|    29|   32|\n",
      "|    26|   25|\n",
      "|    19|   28|\n",
      "|    54|   29|\n",
      "|    22|   37|\n",
      "|     7|   34|\n",
      "|    34|   23|\n",
      "|    50|   29|\n",
      "|    57|   18|\n",
      "|    32|   18|\n",
      "|    43|   28|\n",
      "|    31|   26|\n",
      "|    39|   39|\n",
      "|    25|   33|\n",
      "|     6|   17|\n",
      "|    58|   20|\n",
      "|     9|   21|\n",
      "|    27|   20|\n",
      "|    56|   25|\n",
      "|    51|   18|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the partition number suddenly increases. This is due to the fact that the Spark SQL module contains the following default configuration: spark.sql.shuffle.partitions set to 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your use case, this can be benefitial or harmfull. In this particular case, one can see that the data volume is not enough to fill all the partitions when there are 200 of them, which causes unnecessary map reduce operations, and ultimately causes the creation of very small files in HDFS, which is not desirable.\n",
    "\n",
    "If we want to change this parameter (and many others) we can do it through the SQLContext..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
