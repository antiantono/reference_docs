{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a6ed25-a8b8-4d9c-afb9-8427f659f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('Elephas_App').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d82a74-bfb0-477a-9ee7-c49826475332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Elephas_App>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c69df7d1-df01-4424-90a7-2e17ccb5e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]')\\\n",
    ".appName('deep-learning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0138ecfb-30b4-4a8a-a5e0-e8d51f8c72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "def shuffle_csv(csv_file):\n",
    "    lines = open(csv_file).readlines()\n",
    "    random.shuffle(lines)\n",
    "    open(csv_file, 'w').writelines(lines)\n",
    "\n",
    "def load_data_frame(csv_file, shuffle=True, train=True):\n",
    "    if shuffle:\n",
    "        shuffle_csv(csv_file)\n",
    "    data = sc.textFile(csv_file)\n",
    "    # This is an RDD, which will later be transformed to a data frame\n",
    "    data = data.filter(lambda x:x.split(',')[0] != 'id').map(lambda line: line.split(','))\n",
    "    if train:\n",
    "        data = data.map(\n",
    "            lambda line: (Vectors.dense(np.asarray(line[1:-1]).astype(np.float32)),\n",
    "                          str(line[-1])) )\n",
    "    else:\n",
    "        # Test data gets dummy labels. We need the same structure as in Train data\n",
    "        data = data.map( lambda line: (Vectors.dense(np.asarray(line[1:]).astype(np.float32)),\"Class_1\") ) \n",
    "    return sql_context.createDataFrame(data, ['features', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e68c9f4-a5c5-4be4-9347-1ae068fc0696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame:\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[1.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,1.0,0.0,1.0,...| Class_8|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_5|\n",
      "|[1.0,1.0,0.0,0.0,...| Class_6|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_6|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_6|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test data frame (note the dummy category):\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[2.0,2.0,14.0,16....| Class_1|\n",
      "|[0.0,1.0,12.0,1.0...| Class_1|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[1.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[2.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data_frame(\"data/S8_train.csv\")\n",
    "test_df = load_data_frame(\"data/S8_test.csv\", shuffle=False, train=False) # No need to shuffle test data\n",
    "\n",
    "print(\"Train data frame:\")\n",
    "train_df.show(10)\n",
    "\n",
    "print(\"Test data frame (note the dummy category):\")\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb44953-3098-4371-8d39-71c0b0c4591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "fitted_indexer = string_indexer.fit(train_df)\n",
    "indexed_df = fitted_indexer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9af0bae-5f99-4323-be68-5955bc1906f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of indexing and scaling. Each transformation adds new columns to the data frame:\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|            features|category|label|     scaled_features|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "|[1.0,0.0,0.0,0.0,...| Class_2|  0.0|[0.40208999583479...|\n",
      "|[0.0,1.0,0.0,1.0,...| Class_8|  2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_5|  6.0|[-0.2535060296260...|\n",
      "|[1.0,1.0,0.0,0.0,...| Class_6|  1.0|[0.40208999583479...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_6|  1.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|  0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_6|  1.0|[-0.2535060296260...|\n",
      "+--------------------+--------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "fitted_scaler = scaler.fit(indexed_df)\n",
    "scaled_df = fitted_scaler.transform(indexed_df)\n",
    "print(\"The result of indexing and scaling. Each transformation adds new columns to the data frame:\")\n",
    "scaled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3798e-0fac-45a8-a8fe-78c0c1e6cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed8e502a-bbec-4ad1-982a-b7fe40984614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "nb_classes = train_df.select(\"category\").distinct().count()\n",
    "input_dim = len(train_df.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(input_dim,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f133177a-83ab-45ca-ad84-1c4a2ee06bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_48bd89065c64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "# The next two paramters come directly from pyspark\n",
    "estimator.setFeaturesCol(\"scaled_features\")             \n",
    "estimator.setLabelCol(\"label\")                \n",
    "# Provide serialized Keras model to the estimator object\n",
    "estimator.set_keras_model_config(model.to_yaml())\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(5) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f3d67-5405-44d0-93f8-1d1ab8e61545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d511bcda-060e-4544-bc36-5c29b0c6ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, scaler, estimator])\n",
    "fitted_pipeline = pipeline.fit(train_df) \n",
    "prediction = fitted_pipeline.transform(train_df)\n",
    "# prediction = fitted_pipeline.transform(test_df) # <-- The same code evaluates test data.\n",
    "pnl = prediction.select(\"label\", \"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e728d86a-b9db-4f6e-b22c-ec0466ead928",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o368.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 227) (192.168.13.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in <listcomp>\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1416, in <lambda>\n    return lambda *a: dataType.fromInternal(a)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in fromInternal\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in <listcomp>\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 452, in fromInternal\n    return self.dataType.fromInternal(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 709, in fromInternal\n    return self.deserialize(v)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 163, in deserialize\n    raise ValueError(\"do not recognize type %r\" % tpe)\nValueError: do not recognize type None\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in <listcomp>\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1416, in <lambda>\n    return lambda *a: dataType.fromInternal(a)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in fromInternal\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in <listcomp>\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 452, in fromInternal\n    return self.dataType.fromInternal(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 709, in fromInternal\n    return self.deserialize(v)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 163, in deserialize\n    raise ValueError(\"do not recognize type %r\" % tpe)\nValueError: do not recognize type None\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9777c9978fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o368.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 227) (192.168.13.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in <listcomp>\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1416, in <lambda>\n    return lambda *a: dataType.fromInternal(a)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in fromInternal\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in <listcomp>\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 452, in fromInternal\n    return self.dataType.fromInternal(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 709, in fromInternal\n    return self.deserialize(v)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 163, in deserialize\n    raise ValueError(\"do not recognize type %r\" % tpe)\nValueError: do not recognize type None\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in <listcomp>\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n    yield self._read_with_length(stream)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1416, in <lambda>\n    return lambda *a: dataType.fromInternal(a)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in fromInternal\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 651, in <listcomp>\n    values = [f.fromInternal(v) if c else v\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 452, in fromInternal\n    return self.dataType.fromInternal(obj)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 709, in fromInternal\n    return self.deserialize(v)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 163, in deserialize\n    raise ValueError(\"do not recognize type %r\" % tpe)\nValueError: do not recognize type None\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n"
     ]
    }
   ],
   "source": [
    "pnl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c017a30-a10a-4075-b0ab-cd4b19f8bd21",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o363.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 228) (192.168.13.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1598, in predict\n    data_handler = data_adapter.DataHandler(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1100, in __init__\n    self._adapter = adapter_cls(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 263, in __init__\n    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1016, in _process_tensorlike\n    inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1011, in _convert_numpy_and_scipy\n    return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1404, in convert_to_tensor_v2_with_dispatch\n    return convert_to_tensor_v2(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1410, in convert_to_tensor_v2\n    return convert_to_tensor(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\n    return func(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\n    return constant_op.constant(value, dtype, name=name)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1598, in predict\n    data_handler = data_adapter.DataHandler(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1100, in __init__\n    self._adapter = adapter_cls(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 263, in __init__\n    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1016, in _process_tensorlike\n    inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1011, in _convert_numpy_and_scipy\n    return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1404, in convert_to_tensor_v2_with_dispatch\n    return convert_to_tensor_v2(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1410, in convert_to_tensor_v2\n    return convert_to_tensor(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\n    return func(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\n    return constant_op.constant(value, dtype, name=name)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b734252c867b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmetrics_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmetrics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o363.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 228) (192.168.13.6 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1598, in predict\n    data_handler = data_adapter.DataHandler(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1100, in __init__\n    self._adapter = adapter_cls(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 263, in __init__\n    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1016, in _process_tensorlike\n    inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1011, in _convert_numpy_and_scipy\n    return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1404, in convert_to_tensor_v2_with_dispatch\n    return convert_to_tensor_v2(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1410, in convert_to_tensor_v2\n    return convert_to_tensor(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\n    return func(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\n    return constant_op.constant(value, dtype, name=name)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/elephas/ml_model.py\", line 226, in extract_features_and_predict\n    return model.predict(np.array([from_vector(x[features_col]) for x in data]))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1598, in predict\n    data_handler = data_adapter.DataHandler(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1100, in __init__\n    self._adapter = adapter_cls(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 263, in __init__\n    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1016, in _process_tensorlike\n    inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in map_structure\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n    structure[0], [func(*x) for x in entries],\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1011, in _convert_numpy_and_scipy\n    return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1404, in convert_to_tensor_v2_with_dispatch\n    return convert_to_tensor_v2(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1410, in convert_to_tensor_v2\n    return convert_to_tensor(\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\n    return func(*args, **kwargs)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 52, in _default_conversion_function\n    return constant_op.constant(value, dtype, name=name)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n  File \"/home/philipharman/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.rdd.RDD$$anon$3.hasNext(RDD.scala:951)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.rdd.RDD$$anon$3.foreach(RDD.scala:950)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "metrics_df= prediction.toPandas()\n",
    "labels= metrics_df[\"label\"].values\n",
    "predictions= [np.argmax(val) for val in np.array(metrics_df[\"prediction\"].values)]\n",
    "accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ad716-4fe0-4c55-b378-bcad99ec5afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
